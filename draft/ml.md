---
title: 深度神经网络监督学习任务训练攻略
UpdatedAt: Wed, 9 Jun 2021 15:32:00 +0900
---



最近学习了一些机器学习的训练方法。在这里和大家分享一下。

机器学习的训练过程主要可以分为下面 3 个步骤。

首先，我们需要定义一个函数，确定需要有哪些特征（Feature）作为输入，以及输出什么样的标签（Label）作为输出。

然后，我们需要定一个损失函数，损失函数的输入是模型的参数，通过训练数据，我们可以输出给定参数下的模型是有多好。

最后，我们通过梯度下降等方式，来优化模型的参数，试图找到损失最小的函数，作为最终的模型。

---

在实际操作过程中，经常会出现训练的模型总是不够好，模型的 Loss 总是很高的情况。我认为快速确定自己的模型有什么问题，是非常重要的。今天在这里总结了一些，和大家分享。

我们一般会把数据分为训练集和测试集。使用训练集进行训练，使用测试集测试模型的效果。在调整模型时，我们应该首先观察训练集上的结果。

如果训练集上的结果就很不好，有两种可能的原因：

第一种是 Model Bias。

Model Bias 出现的原因是模型太过于简单。解决的方案是，重新设计模型，让它更加灵活。具体来说，你可以增加更多的 Feature，或者如果使用的是 Deep Learning 的手段的话，可以使用更多的 neurons 和 layers。

另一种更加难以解决，就是 Optimization 的问题。

我们一般都是使用 Gradient descent 这种方式来进行 optimization。Gradient descent 这种方式有一些缺点，它有可能会卡在 Local Minima 或者 Saddle point 的地方，无法找到全局最优解。

如果在训练时，我们就无法训练出 loss 比较低的模型，到底是出现了 Model Bias 的问题，还是 Optimization 的问题呢？

在一篇论文中提到了区分这两个问题的方法。这个论文一开始举了一个例子。某个任务，使用 20 层的神经网络模型时，效果比 56 层的神经网络更好。早期的人们对深度神经网络的了解不够透彻，有一些错误的理解，认为这种是56层的神经网络出现了 overfitting 的问题。但这其实不是 overfitting 的问题，而是 optimization 的问题。因为在训练集上，20 层模型表现得更好。56 层的模型更加灵活，肯定能做到 20 层模型能做到的事情，但是没有。

因此稍微总结一下，如果我怀疑自己的模型是 optimization 有问题，我们可以构建一个层数较少的简单模型，或者我们也可以使用 SVM 等不容易 optimization 失败的模型，来进行对比。如果简单的模型就已经能做得很好，那就说明出现了 optimization 问题。

那么如何解决 optimization 的问题呢？

首先我们需要对 local minima 和 saddle point 区分来看。如果我们遇到了 local minima，那么确实没有办法继续更新模型了。但如果我们遇到的是 saddle point，其实我们还是能够继续更新参数。

在实际训练模型的时候，local minima 其实是很难出现的。原因如下：我举的例子其实只有一个参数，但在实际的训练中，模型的参数非常多，要让所有参数都无法更新是一件概率很小的事情。这边也有一篇论文进行了相关的研究。研究表明模型很难停留在 local minima。

---

如果训练时 Loss 小，测试时 Loss 大，那么真的很有可能遇到了 Overfitting 的问题。

解决 Overfitting 问题，最好的办法其实很简单，那就是增加训练资料的数量。我们可以通过收集更多的数据来解决。如果我们一时无法获得更多的数据，我们也可以进行 Data augmentation，来创造更多的训练资料。比如在图像识别任务中，我们可以左右反转图片，或者裁剪图片，生成更多图像。

另一种方式，是给模型添加一些限制，比如说使用更少的参数、更少的 Feature、Early Stop、正则化。需要注意的是，我们也不能给模型限制太多，如果限制太多，就会回到之前的 Model Bias 问题。


