---
title: ART.T458 - Assignment - 1
UpdatedAt: Wed, 16 Jun 2021 14:32:00 +0900
---

Assignment Shimosaka-1

Consider the following optimization problem:

$$
\hat{a}, \hat{b}, \hat{\sigma}^{2}=\underset{a, b, \sigma^{2}}{\operatorname{argmax}} \sum_{i=1}^{n} \ln p\left(y_{i} \mid x_{i} ; a, b, \sigma^{2}\right)
$$

Training samples: $x_1 = 1,y_1 = 2,x_2 = 2,y_2 = 3,x_3 = 4,y_3 = 6$

Apply maximum likelihood method, we got:

$$
\hat{a}, \hat{b}, \hat{\sigma}^{2}=\underset{a, b, \sigma^{2}}{\operatorname{argmax}}\left(-\frac{n}{2} \ln (2 \pi)-n \ln |\sigma|-\sum_{i=1}^{n} \frac{1}{2} \frac{\left(y_{i}-a x_{i}-b\right)^{2}}{\sigma^{2}}\right)
$$

Since we only care about the value of the parameter $\bold{a}$ and $\bold{b}$ when the likelihood function is maximum, the standard deviation $\sigma$ does not affect the value of maximum, so we can ignore the terms with $\sigma$. Also, for simplicity, I'll add a notation to turn the problem into a minimized problem. The final equation will look like this:

$$
\hat {a}, \hat{b}, \hat{\sigma}^{2}=\argmin_{a,b,\sigma^{2}} \sum^n_{i=1}{\frac{(y_i -ax_i - b)^2}{\sigma^2}}
$$

Namely, minimize loss function $L$:

$$L(a,b) = \sum^n_{i=1}{(y_i - ax_i -b)^2}$$

To minimize the equation, we only need to take the partial derivative of a and b respectively, and minimize it when the derivative is 0:

$$
\frac{\partial L(a,b)}{\partial a} =  2 \sum^n_{i=1}{x_i(ax_i + b - y_i) = 0} \\
\frac{\partial L(a,b)}{\partial b} =  2 \sum^n_{i=1}{ax_i + b - y_i = 0} \\
$$

Clem's rule can be used to solve it, the result is:
$$
\hat b=\frac{\sum x_{i}^{2} \sum y_{i}-\sum x_{i} \sum x_{i} y_{i}}{n \sum x_{i}^{2}-\left(\sum x_{i}\right)^{2}} \\
\hat a=\frac{n \sum x_{i} y_{i}-\sum x_{i} \sum y_{i}}{n \sum x_{i}^{2}-\left(\sum x_{i}\right)^{2}}
$$

The final result can be obtained by substituting data into the equation:

$$
\hat b = 0.5, \hat a \approx 1.36
$$

Given the parameters, we can figure out the expectation. Then the variance is obtained as follows:

$$
\hat \sigma^2 \approx 0.024
$$
